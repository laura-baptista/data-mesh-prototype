services:
  # =======================
  # Infraestrutura base
  # =======================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.0
    profiles: ["kafka", "full", "spark"]
    container_name: zookeeper
    restart: always
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - data-mesh

  kafka:
    image: confluentinc/cp-kafka:7.7.0
    profiles: ["kafka", "full", "spark"]
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"   # interno
      - "9093:9093"   # externo
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      # Dois listeners REAIS
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    healthcheck:
      test: [ "CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "kafka:9092" ]
      interval: 30s
      timeout: 10s
      retries: 4
    networks:
      - data-mesh

  topic-creator:
    profiles: ["kafka", "full", "spark"]
    build: ./kafka
    depends_on:
      - kafka
    restart: "on-failure"
    networks:
      - data-mesh

  # =======================
  # Processamento
  # =======================
  spark:
    profiles: ["full", "spark"]
    image: apache/spark:3.5.1
    build:
        context: ./spark
    container_name: spark
    restart: always
    ports:
      - "7077:7077"
      - "8083:8080"
    command: >
      bash -c "
        /opt/spark/sbin/start-master.sh &&
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-1-*.out
      "
    environment:
      AWS_PROFILE: data-mesh
      AWS_REGION: us-east-1
    volumes:
      - ../scripts/consumers/spark/app:/opt/spark-apps
    networks:
      - data-mesh

  flink:
    profiles: ["full"]
    image: flink:latest # A priori, deixei latest
    container_name: flink
    restart: always
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink
    networks:
      - data-mesh
    
  # =======================
  # Orquestração
  # =======================
  airflow-postgres:
    image: postgres:15
    container_name: airflow-postgres
    profiles: ["airflow", "full"]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./postgres:/var/lib/postgresql/data
    restart: always

  airflow-init:
    image: apache/airflow:2.10.2
    profiles: ["airflow", "full"]
    depends_on:
      - airflow-postgres
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@local --password admin
      "
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-webserver:
    image: apache/airflow:2.10.2
    container_name: airflow-webserver
    profiles: ["airflow", "full"]
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
      - airflow-scheduler
    command: webserver
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-scheduler:
    image: apache/airflow:2.10.2
    container_name: airflow-scheduler
    profiles: ["airflow", "full"]
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
    command: scheduler
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-triggerer:
    image: apache/airflow:2.10.2
    container_name: airflow-triggerer
    profiles: ["airflow", "full"]
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
    command: triggerer
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  # =======================
  # Consulta e Visualização
  # =======================
  dremio:
    profiles: ["full"]
    image: dremio/dremio-oss:latest # A priori, deixei latest
    container_name: dremio
    restart: always
    ports:
      - "9047:9047"
      - "31010:31010"
    volumes:
      - ./dremio:/opt/dremio/data
    networks:
      - data-mesh

  metabase:
    profiles: ["full"]
    image: metabase/metabase
    container_name: metabase
    restart: always
    ports:
      - "3000:3000"
    environment:
      - MB_DB_FILE=/metabase.db
    volumes:
      - ./metabase:/metabase.db
    networks:
      - data-mesh

networks:
  data-mesh:
    name: data-mesh
    driver: bridge
