services:
  # =======================
  # Infraestrutura base
  # =======================
  zookeeper:
    image: wurstmeister/zookeeper
    profiles: ["kafka", "full"]
    container_name: zookeeper
    restart: always
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: wurstmeister/kafka
    profiles: ["kafka", "full"]    
    container_name: kafka
    restart: always
    ports:
      - '9092:9092'
      - '9093:9093'
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_LISTENERS=INTERNAL://:9092,EXTERNAL://:9093
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    depends_on:
      - zookeeper
    healthcheck:
      test: [ "CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "kafka:9092" ]
      interval: 30s
      timeout: 10s
      retries: 4

  topic-creator:
    profiles: ["kafka", "full"]
    build: ./kafka
    depends_on:
      - kafka
    restart: "on-failure"

  # =======================
  # Processamento
  # =======================
  spark:
    profiles: ["full"]
    image: apache/spark:3.5.0
    container_name: spark
    restart: always
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MODE=master
    volumes:
      - ./spark/app:/opt/spark-apps
    networks:
      - data-mesh

  flink:
    profiles: ["full"]
    image: flink:latest # A priori, deixei latest
    container_name: flink
    restart: always
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink
    networks:
      - data-mesh
    
  # =======================
  # Orquestração
  # =======================
  airflow-postgres:
    image: postgres:15
    container_name: airflow-postgres
    profiles: ["airflow", "full"]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./postgres:/var/lib/postgresql/data
    restart: always

  airflow-init:
    image: apache/airflow:2.10.2
    profiles: ["airflow", "full"]
    depends_on:
      - airflow-postgres
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@local --password admin
      "
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-webserver:
    image: apache/airflow:2.10.2
    container_name: airflow-webserver
    profiles: ["airflow", "full"]
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
      - airflow-scheduler
    command: webserver
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-scheduler:
    image: apache/airflow:2.10.2
    container_name: airflow-scheduler
    profiles: ["airflow", "full"]
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
    command: scheduler
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-triggerer:
    image: apache/airflow:2.10.2
    container_name: airflow-triggerer
    profiles: ["airflow", "full"]
    restart: always
    depends_on:
      - airflow-postgres
      - airflow-init
    command: triggerer
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  # =======================
  # Consulta e Visualização
  # =======================
  dremio:
    profiles: ["full"]
    image: dremio/dremio-oss:latest # A priori, deixei latest
    container_name: dremio
    restart: always
    ports:
      - "9047:9047"
      - "31010:31010"
    volumes:
      - ./dremio:/opt/dremio/data
    networks:
      - data-mesh

  metabase:
    profiles: ["full"]
    image: metabase/metabase
    container_name: metabase
    restart: always
    ports:
      - "3000:3000"
    environment:
      - MB_DB_FILE=/metabase.db
    volumes:
      - ./metabase:/metabase.db
    networks:
      - data-mesh

networks:
  data-mesh:
    driver: bridge
